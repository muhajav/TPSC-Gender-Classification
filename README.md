# Gender Classification with CelebA

A project to classify gender from face images and explain the model's decisions using LIME and Saliency Maps. This repository contains the code for training the model and will be used for collaborative development of the explanation scripts.

---

## üìã Project Overview

- **Dataset**: CelebA (Align&Cropped version)
- **Model**: A ResNet18 model, fine-tuned for binary gender classification.
- **Explainability Methods**: LIME vs. Saliency Maps

---

## ‚öôÔ∏è Prerequisites

Before you begin, ensure you have the following installed on your machine:

- Python 3.8 or higher
- Git

---

## üöÄ Setup Instructions

Follow these steps to get a working copy of the project on your local machine.

### 1. Clone the Repository

First, clone the project code from GitHub. Open your terminal and run:

```bash
git clone [https://github.com/muhajav/TPSC-Gender-Classification](https://github.com/muhajav/TPSC-Gender-Classification)
```

### 2. Download the Data and Model

The dataset and the pre-trained model are too large for GitHub and are stored in a shared Google Drive folder.

- **[Click here to download the `data` and `models` folders](https://drive.google.com/drive/folders/1u_7_UV1suI2wwp864qCMfSdzwz--UDSb?usp=sharing)**
- Place both the `data` and `models` folders directly inside the project directory you just cloned.

### 3. Set Up the Python Environment

We use a virtual environment to keep project dependencies separate.

```bash
# Navigate into the project folder
cd TPSC-Gender-Classification

# Create a virtual environment
python3 -m venv venv

# Activate the environment
source venv/bin/activate
```

Your terminal prompt should now start with `(venv)`.

### 4. Install Dependencies

Install all the required Python libraries. You will need to add `lime`, `scikit-image`, and `matplotlib` to the list.

```bash
pip3 install torch torchvision tqdm lime scikit-image matplotlib
```

---

## ü§ù Team Tasks & Workflow

This section outlines the specific tasks for each team member.

### ‚úÖ Javi: Data & Model Setup (Complete)

Javier's task of collecting the data, training the base model, and setting up this repository is complete. The trained model is available at `./models/gender_classifier_model.pth`.

### üë®‚Äçüíª Yudi & Zidni: Apply Explainability Methods

Your goal is to write a script that takes the trained model and an image, and then generates the raw explanation data (heatmaps) for both LIME and Saliency.

1.  **Create a New Script**: Create a file named `explain.py`.
2.  **Load the Model**: Start your script by loading the pre-trained model.

    ```python
    import torch
    # ... (Add other necessary imports)

    # Make sure to load the model structure first, then the weights
    model = ... # (Instantiate the same ResNet18 model from run_training.py)
    model.load_state_dict(torch.load('./models/gender_classifier_model.pth'))
    model.eval() # Set model to evaluation mode
    ```

3.  **Implement Saliency Map**: For a given input image, calculate the Saliency Map. This involves getting the gradient of the output with respect to the input pixels.
4.  **Implement LIME**: Use the `lime` library to generate an explanation for the same image. You will likely use the `LimeImageExplainer`.
5.  **Save the Raw Results**: For each image you test, save the resulting heatmaps as NumPy arrays (e.g., `saliency_image01.npy`, `lime_image01.npy`). This will be the input for Osa and Adams.

### üé® Osa & Adams: Visualize the Results

Your task is to take the raw heatmap data from Yudi and Zidni and create clear, comparative visualizations for the presentation.

1.  **Create a New Script**: Create a file named `visualize.py`.
2.  **Load the Data**: In your script, load an original test image and the corresponding `.npy` heatmap files generated by Yudi and Zidni.
3.  **Generate Visualizations**: Use a library like `matplotlib` to create the final images. Good ideas for visualizations include:
    - A side-by-side plot: [Original Image] | [Saliency Heatmap] | [LIME Explanation].
    - Overlaying the heatmaps on top of the original images with transparency.
4.  **Save the Images**: Save your final plots as high-quality `.png` or `.jpg` files in a new `results/` folder.

### üìä Evan: Compare the Methods & Draw Conclusions

Your role is to analyze the final visualizations and synthesize the findings for the presentation.

1.  **Review the Visuals**: Examine the images created by Osa and Adams.
2.  **Answer the Core Project Questions**: Based on the visuals, formulate answers to the key questions:
    - Do LIME and Saliency highlight the same important features (e.g., jawline, hair, eyes)?
    - Which method provides explanations that are clearer or more intuitive to a human?
    - Are there any limitations or misleading aspects of either method? (e.g., Are the Saliency maps too noisy? Does LIME's output change drastically based on its settings?)
3.  **Write Down Conclusions**: Summarize your findings in a few key bullet points. This summary will be the main narrative for the final presentation slides.
